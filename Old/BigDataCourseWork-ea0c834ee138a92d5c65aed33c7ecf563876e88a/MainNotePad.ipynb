{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:Red;\"> Importing the libaries </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:Green;\"> Sanitisation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45585 entries, 0 to 45584\n",
      "Data columns (total 9 columns):\n",
      "name                45585 non-null object\n",
      "category            45585 non-null object\n",
      "deadline            45585 non-null object\n",
      "launched            45585 non-null object\n",
      "backers             45585 non-null int64\n",
      "country             45585 non-null object\n",
      "usd_pledged_real    45585 non-null float64\n",
      "usd_goal_real       45585 non-null float64\n",
      "StateBin            45585 non-null int64\n",
      "dtypes: float64(2), int64(2), object(5)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "FullSetClean = pd.read_csv(\"data_edited3.csv\")\n",
    "\n",
    "#####\n",
    "\"\"\"\n",
    "Put Further Sanitation Code here.\n",
    "OR any sanitation code\n",
    "\"\"\"\n",
    "FullSetClean = FullSetClean.drop(columns = \"currency\")\n",
    "\n",
    "stateBin = []\n",
    "for row in FullSetClean[\"state\"]:\n",
    "    if row == \"successful\":\n",
    "        stateBin.append(1)\n",
    "    else:\n",
    "        stateBin.append(0)\n",
    "\n",
    "FullSetClean[\"StateBin\"] = stateBin\n",
    "FullSetClean = FullSetClean.drop(columns = \"state\")\n",
    "FullSetClean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:Fuchsia;\"> The functions related to the words </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:Teal;\"> Letter Related Functions </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0.25\n",
      "0.6666666666666666\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def StartingChar(string : str): # discontinued as does not output a numerical number that could be used later.\n",
    "    return string[0]\n",
    "\n",
    "def Length(string : str):\n",
    "    return len(string)\n",
    "\n",
    "def NumberOfWords(string :  str):\n",
    "    output = 0\n",
    "    string = string.split()\n",
    "    for word in string:\n",
    "         if any(c.isalpha() for c in word): ##sees if there is a letter in the collection of chars\n",
    "                output += 1\n",
    "    return output\n",
    "\n",
    "def Capitilisation(string : str):\n",
    "    plus = 0\n",
    "    minus = 0\n",
    "    for char in string:\n",
    "        if char.islower():\n",
    "            plus +=1\n",
    "        elif char.isupper():\n",
    "            minus += 1\n",
    "    return (plus/(plus + minus))\n",
    "\n",
    "def Punctuation(string : str):\n",
    "    plus = 0\n",
    "    minus = 0\n",
    "    for char in string:\n",
    "        if char in \". , / ? ; : ‘ () !”\":\n",
    "            plus +=1\n",
    "        else:\n",
    "            minus += 1\n",
    "    return (plus/(plus + minus))\n",
    "\n",
    "def nonPunctuation(string:str):\n",
    "    plus = 0\n",
    "    minus = 0\n",
    "    for char in string:\n",
    "        if char in \"\\|£$%^&*-_+={}[]@~#<>¬\":\n",
    "            plus +=1\n",
    "        else:\n",
    "            minus += 1\n",
    "    return (plus/(plus + minus))\n",
    "\n",
    "def Vowels(string : str):\n",
    "    plus = 0\n",
    "    minus = 0\n",
    "    string = string.lower()\n",
    "    for char in string:\n",
    "        if char in \"aeiou\":\n",
    "            plus += 1\n",
    "        elif char.isalpha():\n",
    "            minus += 1\n",
    "    return (plus/(plus + minus))\n",
    "\n",
    "def Plositives(string : str):\n",
    "    plus = 0\n",
    "    minus = 0\n",
    "    string = string.lower()\n",
    "    for char in string:\n",
    "        if char in \"ptkbdg\":\n",
    "            plus += 1\n",
    "        elif char.isalpha():\n",
    "            minus += 1\n",
    "    return (plus/(plus + minus))\n",
    "\n",
    "def frictives(string : str):\n",
    "    plus = 0\n",
    "    minus = 0\n",
    "    string  = string.lower()\n",
    "    for char in string:\n",
    "        if char in \"fsvz\":\n",
    "            plus += 1\n",
    "        elif char.isalpha():\n",
    "            minus += 1\n",
    "    return (plus/(plus + minus))\n",
    "\n",
    "def alliteration(string : str):\n",
    "    output = 0\n",
    "    string = string.lower()\n",
    "    string = string.split()\n",
    "    previousLetter = \"\"\n",
    "    output = 0\n",
    "    for word in string:\n",
    "        if word[0] == previousLetter:\n",
    "                output += 1\n",
    "        else:\n",
    "            previousLetter = word[0]\n",
    "        \n",
    "    return output\n",
    "\n",
    "functionList = [\n",
    "                [Length , \"Word Length\"],\n",
    "                [NumberOfWords , \"Number Of Words\"],\n",
    "                [Capitilisation , \"Capitilisation\"],\n",
    "                [Punctuation , \"Punctuation\"],\n",
    "                [nonPunctuation , \"nonPunctuation\"],\n",
    "                [Vowels , \"Vowels\"],\n",
    "                [Plositives, \"Plositives\"],\n",
    "                [frictives,\"frictives\"],\n",
    "                [alliteration,\"alliteration\"]\n",
    "               ]\n",
    "    \n",
    "print(NumberOfWords(\"Test Test test a!\")) \n",
    "print (frictives(\"test test test\"))\n",
    "print (Capitilisation(\"TEst Test Test\"))\n",
    "print (alliteration(\"Fest Test test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At some point this will be changed to a genralised function.\n",
    "#base plan:\n",
    "#all the analytics function will take a string return a number (float or int 64 )\n",
    "\n",
    "#there will be an itterator high order function that takes the analytic function. \n",
    "#and the applies it to all of the titles and returns a list\n",
    "\n",
    "\n",
    "###The itterator high order\n",
    "\n",
    "def itterator(function,ColName,dataSet):\n",
    "    output = list()\n",
    "    for n , string in enumerate(dataSet[\"name\"]):\n",
    "        output.append(function(string))\n",
    "    dataSet[ColName] = output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function Length at 0x7f2a30f56ea0>\n",
      "<function NumberOfWords at 0x7f2a33bb3488>\n",
      "<function Capitilisation at 0x7f2a33bb32f0>\n",
      "<function Punctuation at 0x7f2a33bb3510>\n",
      "<function nonPunctuation at 0x7f2a33bb3268>\n",
      "<function Vowels at 0x7f2a33bb3400>\n",
      "<function Plositives at 0x7f2a33bb3620>\n",
      "<function frictives at 0x7f2a33bb38c8>\n",
      "<function alliteration at 0x7f2a33bb3950>\n",
      "done functions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for point in functionList:\n",
    "    itterator(point[0],point[1],FullSetClean)\n",
    "    print(point[0])\n",
    "\n",
    "print (\"done functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:Orange;\"> Time Related Functions </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def dateTimeitterator(dataSet):\n",
    "    Launchweekdays = []\n",
    "    Launchhours = []\n",
    "    elapsedDays= []\n",
    "    deadlineweekdays = []\n",
    "    draw = []\n",
    "    sraw = []\n",
    "    \n",
    "    for n , string in enumerate(dataSet[\"launched\"]):\n",
    "        date , time = string.split(\" \")\n",
    "        day , month ,year = date.split(\"/\")\n",
    "        hour, mineut = time.split(\":\")\n",
    "        raw = datetime.date(int(year), int(month), int(day))\n",
    "\n",
    "        \n",
    "        Launchweekdays.append(int(raw.weekday()))\n",
    "        Launchhours.append(int(hour))\n",
    "        sraw.append(raw)\n",
    "        \n",
    "    for n , string in enumerate(dataSet[\"deadline\"]):\n",
    "        \n",
    "        day , month ,year = string.split(\"/\")\n",
    "        hour, mineut = time.split(\":\")\n",
    "        raw = datetime.date(int(year), int(month), int(day))\n",
    "        deadlineweekdays.append(raw.weekday())\n",
    "        draw.append(raw)\n",
    "        \n",
    "\n",
    "    elapsedDays = [(a - b).days for a, b in zip(sraw, draw)]\n",
    "    dataSet[\"LaunchWeekday\"] = Launchweekdays\n",
    "    dataSet[\"LaunchHour\"] =  Launchhours\n",
    "    dataSet[\"elapsedDay\"] = elapsedDays\n",
    "    dataSet[\"deadlineWeekday\"] = deadlineweekdays\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done Timeritterato\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dateTimeitterator(FullSetClean)\n",
    "print (\"done Timeritterato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:salmon;\"> Data Base Splitting (to be added data base correction) + Dataset Balancing </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data base splitting done bellow \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train,other = train_test_split(FullSetClean, test_size=0.8,random_state=0);\n",
    "\n",
    "validation , test = train_test_split(other, test_size=0.5,random_state=0);\n",
    "\n",
    "train.head()\n",
    "\n",
    "# FullSetClean.head()\n",
    "# FullSetClean.info(verbose = True)\n",
    "\n",
    "# BALANCES THE TRAINING DATASET:\n",
    "\n",
    "total = len(train)\n",
    "nb_pos = train['StateBin'].sum()\n",
    "nb_neg = total - nb_pos\n",
    "\n",
    "success_pos = train.loc[train['StateBin'] == 1]\n",
    "success_neg = train.loc[train['StateBin'] == 0].sample(nb_pos)\n",
    "\n",
    "resampled_train = pd.concat((success_pos, success_neg))\n",
    "\n",
    "# CHECKS THAT RESAMPLING HAS BEEN SUCCESSFUL:\n",
    "\n",
    "# total = len(resampled_train)\n",
    "# nb_pos = resampled_train['StateBin'].sum()\n",
    "# nb_neg = total - nb_pos\n",
    "\n",
    "# print('Successful: {}' .format(nb_pos))\n",
    "# print('Failed: {}' .format(nb_neg))\n",
    "\n",
    "# BALANCES THE VALIDATION DATASET:\n",
    "\n",
    "total = len(validation)\n",
    "nb_pos = validation['StateBin'].sum()\n",
    "nb_neg = total - nb_pos\n",
    "\n",
    "success_pos = validation.loc[validation['StateBin'] == 1]\n",
    "success_neg = validation.loc[validation['StateBin'] == 0].sample(nb_pos)\n",
    "\n",
    "resampled_validation = pd.concat((success_pos, success_neg))\n",
    "\n",
    "# CHECKS THAT RESAMPLING HAS BEEN SUCCESSFUL:\n",
    "\n",
    "# total = len(resampled_validation)\n",
    "# nb_pos = resampled_validation['StateBin'].sum()\n",
    "# nb_neg = total - nb_pos\n",
    "\n",
    "# print('Successful: {}' .format(nb_pos))\n",
    "# print('Failed: {}' .format(nb_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:Purple;\"><i> Now for the word proccessing </i></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ghgh', 'ghgh', 'fdkfkf', 'g']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def sanitiser(title : str) -> list: #takes a string splits into words , makes lower case and removes punctuation\n",
    "    words = title.split()\n",
    "    output = []\n",
    "    for word in words:\n",
    "       output.append(re.sub(r'\\W+', '', word).lower())\n",
    "    return output\n",
    "\n",
    "def WordFinder(dataSet,cutoff : int) -> dict and list: #so takes in the data set , a cut off an renturns a list of all words above the cutoff and their percentage chance of succsess\n",
    "    LargeWordDictionary = {} #The dict for all of the words \n",
    "    \n",
    "    for n , title in enumerate(dataSet[\"name\"]): #goes through the big old list\n",
    "        temp = sanitiser(title) #sanitises the function\n",
    "        for word in temp: #basically sees if the word is already in the large list of words if it is then it adds its location in the data base to the end of the dict entry\n",
    "            try:\n",
    "                LargeWordDictionary[word].append(n)\n",
    "            except KeyError:\n",
    "                LargeWordDictionary[word] = [n]\n",
    "                \n",
    "    SmallWordDictionary = {} #small output dictionary\n",
    "    StateBin = dataSet[\"StateBin\"].tolist() #transfers database to list due to pandas related issues\n",
    "    for word in LargeWordDictionary: # goes through large dictionary , counts number of instances of each word appearing , and then uses the pointers to find if they were success \n",
    "        Suc = 0\n",
    "        for pointer in LargeWordDictionary[word]:\n",
    "            Suc += StateBin[int(pointer)]\n",
    "        length = len(LargeWordDictionary[word])\n",
    "        if length >= cutoff:\n",
    "            SmallWordDictionary[word] = [length , Suc/length]\n",
    "    print(len(SmallWordDictionary))\n",
    "    \n",
    "    return SmallWordDictionary\n",
    "    #return LargeWordDictionary\n",
    "\n",
    "\n",
    "def WordScore(title : str,AssementDictionary : dict ,split : int) ->list:\n",
    "    title = sanitiser(title)\n",
    "    output = [0 for i in range(split)]\n",
    "    unique = 0\n",
    "    for word in title:\n",
    "        try:\n",
    "            temp = AssementDictionary[word][1]\n",
    "            for i in range(split):\n",
    "                if temp > 1 *((i+1)/split):\n",
    "                    pass\n",
    "                else:\n",
    "                    output[i] += 1\n",
    "                    break\n",
    "        except KeyError:\n",
    "            unique += 1\n",
    "    return output , unique\n",
    "\n",
    "\n",
    "def wordItterator(dataset , split): #### UNUSED FUNCTION AT THE MOMENT\n",
    "    output = []\n",
    "    for n , title in enumerate(dataSet[\"name\"]):\n",
    "        output.append(WordScore(title,dataset,assmentDict,split))\n",
    "    output = np.transpose(output)\n",
    "    for n , row in enumerate(output):\n",
    "        dataset[str(n)] = row\n",
    "        \n",
    "        \n",
    "    \n",
    "def wordWorst(title : str , AssementDict : dict):\n",
    "    title = sanitiser(title)\n",
    "    unique = 0\n",
    "    for word in title:\n",
    "        try:\n",
    "            temp = AssementDict[word][1]\n",
    "            if temp > maximum:\n",
    "                maximum = temp\n",
    "            elif temp < minimum:\n",
    "                minimum = temp\n",
    "        except KeyError:\n",
    "            unique += 1\n",
    "        except NameError:\n",
    "            maximum , minimum = temp , temp\n",
    "    \n",
    "    try:\n",
    "        return maximum , minimum , unique\n",
    "    except NameError:\n",
    "        return 0.5 , 0.5 , unique #There may be a better way of dealig with this but this just an easy way to deal with it.\n",
    "    \n",
    "    \n",
    "def wordWorstItterator(dataSet,AssmentDict):\n",
    "    maximum = []\n",
    "    minimum = []\n",
    "    unique = []\n",
    "    TotalFailures = 0\n",
    "    for n , title in enumerate(dataSet[\"name\"]):\n",
    "        temp = wordWorst(title ,AssmentDict )\n",
    "        maximum.append(temp[0])\n",
    "        minimum.append(temp[1])\n",
    "        unique.append(temp[2])\n",
    "        if temp[0] > 0.7 and temp[1] < 0.3:\n",
    "            TotalFailures += 1\n",
    "    dataSet[\"maximum\"] = maximum\n",
    "    dataSet[\"minimum\"] = minimum\n",
    "    dataSet[\"unique\"] = unique\n",
    "    print(TotalFailures)\n",
    "    \n",
    "sanitiser(\"ghgh ghgh!!!!  FDKFKF  g\")\n",
    "\n",
    "#728 2633 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n",
      "36\n",
      "99\n",
      "118\n",
      "[[0, []], [1, ['app', 'clothing', 'online', 'beauty', 'media', 'startup', 'social', 'based', 'care']], [2, ['truck', 'food', 'brand', 'radio', 'my', 'tshirt', 'website', 'jewelry', 'products', 'healthy', 'shop', 'tv', 'fitness', 'apparel', 'want', 'trump', 'network', 'platform', 'escape', 'fashion', 'line', 'learning', 'y', 'education', 'community', 'mind', 'business', 'than', 'iphone', 'christian']], [3, ['phone', 'n', 'living', 'innovative', 'after', 'mobile', 'how', 'people', 'android', 'friends', 'life', 'handmade', 'sports', 'en', 'de', 'case', 'live', 'system', 'el', 'revolution', 'virtual', 'painting', 'records', 'toy', 'movie', 'journey', 'vegan', 'wear', 'star', 'guide', 'body', 'organic', 'club', 'bar', 'me', 'cafe', 'garden', 'building', 'making', 'vr', 'power', 'natural', 'can', 'start', 'america', 'dog', 'fan', 'free', 'local', 'sustainable', 'take', 'what', 'restaurant', 'photo', 'audio', 'urban', 'lets', 'dreams', 'just', 'children', 'survival', 'italian', 'tshirts', 'safety', 'personal', 'come', 'bakery', 'center', 'arts', 'public', 'or', 'vs', 'accessories']], [4, ['your', 'for', 'to', 'fun', 'family', 'with', 'art', 'summer', 'and', 'water', 'project', 'experience', 'out', 'creative', 'device', 'die', 'be', 'man', 'design', 'custom', 'help', 'get', 'on', 'bear', 'technology', 'story', 'pet', 'funding', '10', 'it', 'small', 'home', 'where', 'dogs', 'build', 'music', 'video', 'books', 'kids', 'back', 'bring', 'real', 'more', 'youth', 'la', 'modern', 'fund', 'we', 'our', 'way', 'pro', 'coffee', 'world', 'usa', 'young', 'great', 'podcast', 'photography', 'reality', 'safe', 'health', 'wood', 'freedom', 'bottle', 'electric', 'kitchen', 'shoes', 'run', 'stand', 'its', 'heroes', 'company', 'ice', 'end', 'launch', 'hand', 'every', 'double', 'car', 'recording', 'tiny', 'wall', 'pilot', 'create', 'designs', 'room', 'i', 'gallery', 'unique', 'animated', 'support', 'air', 'never', 'led', 'baby', 'when', 'sweet', 'soul', 'beer', 'school', 'cream', 'open', 'chocolate', 'hot', 'sauce', 'fidget', 'dream', 'needs', 'poetry', 'money', 'his', 'simple', 'through', 'training', 'planner', 'stop', 'need', 'amazing', 'musical', 'sound', 'christmas', 'change', 'toys', 'fire']], [5, ['a', 'from', '', 'no', 'turn', 'into', 'card', 'game', 'the', 'legacy', 'beyond', 'in', 'an', 'farm', 'new', 'is', 'secret', 'history', 'of', 'gaming', 'novel', 'box', 'better', 'house', 'wearable', 'are', 'you', 'glass', 'mystery', 'digital', 'only', 'at', 'feature', 'documentary', 'production', 'not', 'plus', 'big', 'years', 'love', 'one', 'do', 'true', 'so', 'human', '4', 'book', 'automatic', 'series', 'show', 'original', 'shirts', 'travel', 'bags', 'red', 'death', 'own', 'robot', 'studio', 'luxury', 'bicycle', 'green', 'old', '3d', 'campaign', 'smartphone', 'light', 'hope', 'play', 'their', 'tour', 'easy', 'time', 'outdoor', 'this', 'learn', 'wooden', 'magnetic', 'heart', 'nyc', 'made', 'that', 'magazine', 'next', 'cat', '1st', 'us', 'london', 'childrens', 'quality', 'horror', 'gear', 'style', 'american', 'inspired', 'will', 'up', 'fall', 'handcrafted', 'things', 'future', 'days', 'high', 'keep', 'single', 'ninja', 'pillow', 'york', 'picture', 'movement', 'best', 'ever', 'key', 'black', 'gift', 'look', 'stories', 'night', 'universal', 'pain', 'dont', 'war', 'save', 'wireless', 'brewing', 'am', 'culture', 'bike', 'map', 'dungeon', 'stickers', 'theatre', 'go', 'artist', 'solar', 'everyone', 'x', 'student', 'coin', 'wars', 'tool', 'cats', 'off', 'rock', 'happy', 'song', 'los', 'jazz', 'con']], [6, ['kit', 'co', 'leather', 'smart', 'last', '2017', '2', 'all', 'level', 'short', 'film', 'two', 'comic', 'part', 'by', 'full', 'action', 'as', 'legends', 'dark', 'board', 'tea', 'earth', 'dead', 'little', 'party', 'make', '100', 'collective', 'festival', 'band', '1', 'games', 'release', 'adventure', 'fight', 'collection', 'swiss', 'watches', 'any', 'worlds', 'first', 'bag', 'women', 'space', 'three', 'table', 'most', 'mini', 'blue', 'top', 'lost', 'city', 'tales', 'magic', 'spinner', 'side', 'pocket', 'le', 'affordable', 'relaunch', 'like', 'adventures', 'prints', 'dress', 'dance', 'artists', 'zero', 'white', 'machine', '20', 'ultimate', 'without', 'football', 'golf', 'comics', 'portable', 'print', 'watch', 'cd', 'dragon', '5', 'good', 'day', 'who', 'make100', 'special', 'monster', 'comedy', 'songs', 'presents', 'journal', 'calendar', 'perfect', 'work', 'girl', 'graphic', 'visual', 'now', 'men', 'color', 'moon', 'animal', 'plush', 'web', 'road', 'pop', 'length', 'fiction', 'designed', 'down', 'science', 'forest', 'sky', 'beautiful', 'performance', '7', 'land', 'edc']], [7, ['premium', 'classic', 'set', 'debut', 'album', 'playing', 'cards', 'season', 'rpg', 'ep', 'pen', 'record', 'second', '3', 'enamel', 'pin', 'pins', 'ii', 'solo', 'pack', 'animals', 'other', 'wallet', 'metal', 'anthology', 'expansion', 'place', 'poster', 'silver', 'modular', 'tabletop', 'hard', '2018', 'anniversary', 'limited', 'backpack', 'exhibition', 'printed', 'about', 'volume', 'gold', 'everyday', 'year', 'press', 'tarot', 'vol', 'coloring', 'super', 'illustrated', 'yoga']], [8, ['dragons', 'fantasy', 'edition', 'issue', 'miniatures', 'scifi', 'pi', 'wild', 'vinyl', 'slim', 'monsters', 'roleplaying', 'titanium', 'minimalist', 'camera', 'girls', 'deck', 'kickstarter', 'king', 'sea', 'terrain']], [9, ['dice', '2nd', '28mm']]]\n",
      "([0, 0, 1, 0, 0, 0, 0, 0, 0, 1], 1)\n"
     ]
    }
   ],
   "source": [
    "TrainAll = WordFinder(resampled_train,20)\n",
    "\n",
    "wordWorstItterator(resampled_train,TrainAll)\n",
    "wordWorstItterator(resampled_validation,TrainAll)\n",
    "wordWorstItterator(test,TrainAll)\n",
    "\n",
    "print(TrainClasses)\n",
    "\n",
    "print (WordScore(\"miniatures blank food\",TrainAll,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:DodgerBlue;\"> Now For the Graphs </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentageAtIntervalFloat(dataSet ,varible : str,splits : int) -> list :\n",
    "    #create the search thing\n",
    "    minimum = dataSet[varible].min()\n",
    "    maximum = dataSet[varible].max()\n",
    "    print(minimum , maximum)\n",
    "    difference = maximum - minimum\n",
    "    space = difference / splits\n",
    "    output = [[a,[0,0]] for a in np.arange(minimum,maximum,space)]\n",
    "    testLevels = [a for a in np.arange(minimum,maximum,space)]\n",
    "    Values = dataSet[varible].tolist()\n",
    "    successes = dataSet[\"StateBin\"].tolist()\n",
    "    \n",
    "    for Value , success in zip(Values,successes):\n",
    "        for n , level in enumerate(testLevels):\n",
    "            if Value <= level:\n",
    "                if  success == 1:\n",
    "                    output[n][1][1] += 1\n",
    "                else:\n",
    "                    output[n][1][0] += 1\n",
    "                break\n",
    "    \n",
    "    percentOutput = []\n",
    "    for level in output:\n",
    "        if level[1][1] == 0:\n",
    "            percentOutput.append(0)\n",
    "        else:\n",
    "            percentOutput.append(level[1][1]/(level[1][1]+level[1][0]))\n",
    "    return percentOutput\n",
    "\n",
    "def percentageAtIntervalInt(dataSet ,varible : str) -> list : ###Recode so that it works with intergers\n",
    "    #create the search thing\n",
    "    minimum = dataSet[varible].min()\n",
    "    maximum = dataSet[varible].max()\n",
    "    difference = maximum - minimum\n",
    "    output = [[a,[0,0]] for a in np.arange(minimum,maximum)]\n",
    "    testLevels = [a for a in np.arange(minimum,maximum)]\n",
    "    Values = dataSet[varible].tolist()\n",
    "    successes = dataSet[\"StateBin\"].tolist()\n",
    "    \n",
    "    for Value , success in zip(Values,successes):\n",
    "        for n , level in enumerate(testLevels):\n",
    "            if Value <= level:\n",
    "                if  success == 1:\n",
    "                    output[n][1][1] += 1\n",
    "                else:\n",
    "                    output[n][1][0] += 1\n",
    "                break\n",
    "    \n",
    "    percentOutput = []\n",
    "    for level in output:\n",
    "        if level[1][1] == 0:\n",
    "            percentOutput.append(0)\n",
    "        else:\n",
    "            percentOutput.append(level[1][1]/(level[1][1]+level[1][0]))\n",
    "    return percentOutput\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Length\n",
      "1 60\n",
      "Number Of Words\n",
      "1 16\n",
      "Capitilisation\n",
      "0.0 1.0\n",
      "Punctuation\n",
      "0.0 0.5666666666666667\n",
      "nonPunctuation\n",
      "0.0 0.46153846153846156\n",
      "Vowels\n",
      "0.0 1.0\n",
      "Plositives\n",
      "0.0 1.0\n",
      "frictives\n",
      "0.0 0.6666666666666666\n",
      "alliteration\n",
      "0 6\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "test[\"Vowels\"].max()\n",
    "for n , Fun in enumerate(functionList):\n",
    "\n",
    "    print(Fun[1])\n",
    "    LookList = percentageAtIntervalFloat(FullSetClean , Fun[1],20)\n",
    "    plt.subplot(3,3,n + 1)\n",
    "    plt.plot(LookList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:Navy;\"> Machine Learning Code Taken From Tutorial </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7432 entries, 45175 to 38111\n",
      "Data columns (total 19 columns):\n",
      "category           7432 non-null int64\n",
      "country            7432 non-null object\n",
      "usd_goal_real      7432 non-null float64\n",
      "Word Length        7432 non-null int64\n",
      "Number Of Words    7432 non-null int64\n",
      "Capitilisation     7432 non-null float64\n",
      "Punctuation        7432 non-null float64\n",
      "nonPunctuation     7432 non-null float64\n",
      "Vowels             7432 non-null float64\n",
      "Plositives         7432 non-null float64\n",
      "frictives          7432 non-null float64\n",
      "alliteration       7432 non-null int64\n",
      "LaunchWeekday      7432 non-null int64\n",
      "LaunchHour         7432 non-null int64\n",
      "elapsedDay         7432 non-null int64\n",
      "deadlineWeekday    7432 non-null int64\n",
      "maximum            7432 non-null float64\n",
      "minimum            7432 non-null float64\n",
      "unique             7432 non-null int64\n",
      "dtypes: float64(9), int64(9), object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "###The bit of the code when we standardise the data\n",
    "\n",
    "resampled_train.drop(columns=[\"name\",\"usd_pledged_real\",\"launched\",\"backers\",\"deadline\"],inplace=True)\n",
    "resampled_validation.drop(columns=[\"name\",\"usd_pledged_real\",\"launched\",\"backers\",\"deadline\"],inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "X_train = resampled_train.drop(columns = \"StateBin\")\n",
    "Y_train = resampled_train[\"StateBin\"]\n",
    "\n",
    "X_validation = resampled_validation.drop(columns=\"StateBin\")\n",
    "Y_validation = resampled_validation[\"StateBin\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Film & Video' 'Mixed Media' 'Art' 'Documentary' 'Country & Folk'\n",
      " 'Illustration' 'Playing Cards' 'Dance' 'Tabletop Games' 'Restaurants'\n",
      " 'Hip-Hop' 'Accessories' 'Electronic Music' 'Apparel' 'Fine Art' 'Drama'\n",
      " '3D Printing' 'Apps' 'Gadgets' 'Product Design' 'Experimental'\n",
      " 'Comic Books' 'Webseries' \"Children's Books\" 'Jazz' 'Music' 'Indie Rock'\n",
      " 'Footwear' 'Textiles' 'Games' 'Food Trucks' 'Community Gardens' 'Design'\n",
      " 'Cookbooks' 'Publishing' 'Crafts' 'Gaming Hardware' 'Pop' 'Jewelry'\n",
      " 'Video Games' 'Poetry' 'Metal' 'Sound' 'DIY' 'Technology' 'World Music'\n",
      " 'Hardware' 'Graphic Design' 'Classical Music' 'Video' 'Art Books'\n",
      " 'Animation' 'Zines' 'Fiction' 'Robots' 'Musical' 'Spaces' 'Mobile Games'\n",
      " 'Rock' 'Faith' 'Shorts' 'Food' 'Ready-to-wear' 'Nature' 'Journalism'\n",
      " 'Nonfiction' 'Wearables' 'Woodworking' 'Web' 'Software' 'Anthologies'\n",
      " 'Photo' 'Plays' 'Small Batch' 'Theater' \"Farmer's Markets\"\n",
      " 'Conceptual Art' 'Horror' 'Movie Theaters' 'Literary Spaces' 'Comics'\n",
      " 'Crochet' 'Painting' 'DIY Electronics' 'Performances' 'Digital Art'\n",
      " 'Installations' 'Events' 'Print' 'Graphic Novels' 'Television'\n",
      " 'Photobooks' 'Workshops' 'Science Fiction' 'Public Art' 'Comedy' 'Places'\n",
      " 'Video Art' 'Music Videos' 'Fantasy' 'Live Games' 'Photography'\n",
      " 'Webcomics' 'Punk' 'Performance Art' 'Camera Equipment' 'Festivals'\n",
      " 'Drinks' 'Vegan' 'Flight' 'Pet Fashion' 'Ceramics' 'Audio'\n",
      " 'Space Exploration' 'Literary Journals' 'Knitting' 'Thrillers'\n",
      " 'Sculpture' 'Couture' 'Young Adult' 'Radio & Podcasts' 'Architecture'\n",
      " 'Candles' 'People' 'Printing' 'R&B' 'Interactive Design' 'Letterpress'\n",
      " 'Makerspaces' 'Pottery' 'Narrative Film' 'Fabrication Tools' 'Action'\n",
      " 'Academic' 'Farms' 'Stationery' 'Childrenswear' 'Calendars' 'Family'\n",
      " 'Periodicals' 'Immersive' 'Embroidery' 'Puzzles' 'Typography'\n",
      " 'Translations' 'Glass' 'Bacon' 'Civic Design' 'Animals' 'Quilts' 'Blues'\n",
      " 'Romance' 'Latin' 'Weaving' 'Kids' 'Residencies' 'Taxidermy']\n",
      "['Periodicals' 'Documentary' 'Rock' 'Accessories' 'Dance' 'Product Design'\n",
      " \"Children's Books\" 'Television' 'Games' 'Small Batch' 'Jazz'\n",
      " 'Playing Cards' 'Classical Music' 'Conceptual Art' 'DIY Electronics'\n",
      " 'Punk' 'Music' 'Shorts' 'Film & Video' 'Comedy' 'Fiction' 'Stationery'\n",
      " 'Hardware' 'Design' 'Painting' 'Faith' 'Electronic Music'\n",
      " 'Tabletop Games' 'Technology' 'Gadgets' 'Mobile Games' 'DIY' 'Hip-Hop'\n",
      " 'Video Games' 'Comic Books' 'Nonfiction' 'Comics' 'Apparel' 'Sculpture'\n",
      " 'Indie Rock' 'Kids' 'Drinks' 'Illustration' 'Food' 'Woodworking' 'Blues'\n",
      " 'Publishing' 'Zines' 'Jewelry' 'Science Fiction' 'Webcomics'\n",
      " 'Country & Folk' 'Drama' 'Anthologies' 'Art' 'Plays' 'Restaurants'\n",
      " 'Performances' 'Graphic Novels' 'Festivals' 'Art Books' 'Webseries'\n",
      " 'Radio & Podcasts' 'Photobooks' 'Musical' 'Textiles' 'Photography' 'Web'\n",
      " 'Performance Art' 'Theater' 'People' 'Residencies' 'Thrillers' 'Apps'\n",
      " 'Footwear' 'Farms' 'Sound' 'Crafts' 'Candles' 'Digital Art'\n",
      " 'Narrative Film' 'Romance' 'Wearables' 'Animation' 'Print' 'Pop'\n",
      " 'Calendars' 'Printing' 'Mixed Media' 'Horror' 'World Music' 'Audio'\n",
      " 'Immersive' 'Public Art' 'Civic Design' 'Young Adult' 'Graphic Design'\n",
      " 'Music Videos' '3D Printing' 'Knitting' 'Live Games' 'Literary Journals'\n",
      " 'Ready-to-wear' 'Camera Equipment' 'Places' 'Puzzles' 'Gaming Hardware'\n",
      " 'Spaces' 'Vegan' 'R&B' 'Poetry' 'Fabrication Tools' 'Academic'\n",
      " 'Childrenswear' 'Ceramics' 'Installations' 'Makerspaces' 'Video'\n",
      " 'Experimental' 'Metal' 'Fashion' 'Space Exploration' 'Quilts' 'Fantasy'\n",
      " 'Cookbooks' 'Architecture' 'Fine Art' 'Software' 'Family' 'Food Trucks'\n",
      " 'Robots' 'Nature' 'Typography' 'Latin' 'Pottery' 'Video Art' 'Journalism'\n",
      " 'Community Gardens' 'Events' \"Farmer's Markets\" 'Weaving' 'Pet Fashion'\n",
      " 'Workshops' 'Interactive Design' 'Flight' 'Letterpress' 'Literary Spaces'\n",
      " 'Embroidery' 'Translations' 'Action' 'Couture' 'Crochet' 'Movie Theaters'\n",
      " 'Animals' 'Photo' 'Glass' 'Bacon']\n"
     ]
    }
   ],
   "source": [
    "print(X_test[\"category\"].unique())\n",
    "print(X_train[\"category\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 'Chiptune'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36m_encode_python\u001b[0;34m(values, uniques, encode)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Chiptune'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-03972a8e9a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mX_validation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_validation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_validation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#X_validation.drop(columns=str(col),inplace=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(values, uniques, encode)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \"\"\"\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_encode_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_encode_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36m_encode_python\u001b[0;34m(values, uniques, encode)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             raise ValueError(\"y contains previously unseen labels: %s\"\n\u001b[0;32m---> 71\u001b[0;31m                              % str(e))\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: 'Chiptune'"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].dtype == \"int64\" or X_train[col].dtype == \"float64\":\n",
    "        X_means = X_train[col].mean(axis=0)\n",
    "        X_stds = X_train[col].std(axis=0)\n",
    "\n",
    "        # Standardise the splits.\n",
    "        X_train[col] = (X_train[col] - X_means) / X_stds\n",
    "        X_validation[col] = (X_validation[col] - X_means) / X_stds\n",
    "        X_test[col] = (X_test[col] - X_means) / X_stds\n",
    "    else:\n",
    "        pass\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(list(X_train[col].unique()))\n",
    "        X_train[col] = le.transform(X_train[col]) \n",
    "        X_validation[col] = le.transform(X_validation[col])\n",
    "        X_test[col] = le.transform(X_validation[col])\n",
    "        #X_validation.drop(columns=str(col),inplace=True)\n",
    "\n",
    "\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def select_column_to_add(X_train, y_train, X_val, y_val, columns_in_model, columns_to_test):\n",
    "    \n",
    "    column_best = None\n",
    "    columns_in_model = list(columns_in_model)\n",
    "    \n",
    "    if len(columns_in_model) == 0:\n",
    "        acc_best = 0\n",
    "    elif len(columns_in_model) == 1:\n",
    "        mod = LogisticRegression(C=1e9).fit(X_train[columns_in_model].values.reshape(-1, 1), y_train)\n",
    "        acc_best = accuracy_score(y_val, mod.predict(X_val[columns_in_model].values.reshape(-1, 1)))\n",
    "    else:\n",
    "        mod = LogisticRegression(C=1e9).fit(X_train[columns_in_model], y_train)\n",
    "        acc_best = accuracy_score(y_val, mod.predict(X_val[columns_in_model]))\n",
    "\n",
    "    \n",
    "    for column in columns_to_test:\n",
    "        mod = LogisticRegression(C=1e9).fit(X_train[columns_in_model+[column]], y_train)\n",
    "        y_pred = mod.predict(X_val[columns_in_model+[column]])\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        \n",
    "        if acc - acc_best >= 0.001:  # one of our stopping criteria\n",
    "            acc_best = acc\n",
    "            column_best = column\n",
    "        \n",
    "    if column_best is not None:  # the other stopping criteria\n",
    "        print('Adding {} to the model'.format(column_best))\n",
    "        print('The new best validation accuracy is {}'.format(acc_best))\n",
    "        columns_in_model_updated = columns_in_model + [column_best]\n",
    "    else:\n",
    "        print('Did not add anything to the model')\n",
    "        columns_in_model_updated = columns_in_model\n",
    "    \n",
    "    return columns_in_model_updated, acc_best , mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_test = set(X_train)\n",
    "full_list = set(X_train)\n",
    "\n",
    "columns_in_model = list();\n",
    "\n",
    "for i in range(10):\n",
    "    columns_in_model , acc , LogicalRegression = select_column_to_add(X_train,Y_train,X_validation,Y_validation,columns_in_model,columns_to_test)    \n",
    "    columns_to_test = full_list.difference(columns_in_model)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Tree Code </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib qt \n",
    "\n",
    "best_model = None\n",
    "max_validation_accuracy = 0\n",
    "\n",
    "AccuracyTraining = []\n",
    "AccuracyValidation = []\n",
    "ka = []\n",
    "\n",
    "for k in range(1):\n",
    "    model = RandomForestClassifier(random_state=2, n_estimators=14, max_depth=9)\n",
    "    ka.append(k+1)\n",
    "    \n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_train)\n",
    "    accuracy = accuracy_score(Y_train, y_pred)\n",
    "    AccuracyTraining.append(accuracy)\n",
    "    \n",
    "    y_pred = model.predict(X_validation)\n",
    "    accuracy = accuracy_score(Y_validation, y_pred)\n",
    "    AccuracyValidation.append(accuracy)\n",
    "   \n",
    "\n",
    "    print('Number of trees: {}, accuracy: {} '.format(k, accuracy))\n",
    "    if accuracy > max_validation_accuracy:\n",
    "        max_validation_accuracy = accuracy\n",
    "        best_model = model\n",
    "\n",
    "print('Optimal number of trees: {}'.format(len(best_model.estimators_)))\n",
    "\n",
    "plt.plot(ka,AccuracyTraining)\n",
    "plt.plot(ka,AccuracyValidation)\n",
    "\n",
    "Forrest = best_model\n",
    "\n",
    "#Confusion Matrix\n",
    "y_pred = Forrest.predict(X_validation)\n",
    "accuracy = accuracy_score(Y_validation, y_pred)\n",
    "confusion_matrix(Y_validation, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = best_model.estimators_[10]\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "# Export as dot file\n",
    "predictors = X_train.columns\n",
    "\n",
    "export_graphviz(estimator, out_file='tree.dot', \n",
    "                feature_names = predictors,\n",
    "                class_names = ('Negative', 'Positive'),\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n",
    "\n",
    "# Convert to png using system command (requires Graphviz)\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'nope.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "Image(filename = 'nope.png')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "AccuracyTraining = []\n",
    "AccuracyValidation = []\n",
    "Depth = []\n",
    "\n",
    "# your code here\n",
    "for i in range(1):\n",
    "    Depth.append(i + 1)\n",
    "    model2 = DecisionTreeClassifier(random_state = 1 ,max_depth = 9) # Our classification tree\n",
    "    model2 = model2.fit(X_train, Y_train)\n",
    "    \n",
    "    y_pred = model2.predict(X_train)\n",
    "    accuracy = accuracy_score(Y_train, y_pred)\n",
    "    AccuracyTraining.append(accuracy)\n",
    "    \n",
    "    y_pred = model2.predict(X_validation)\n",
    "    accuracy = accuracy_score(Y_validation, y_pred)\n",
    "    AccuracyValidation.append(accuracy)\n",
    "    print(accuracy)\n",
    "    \n",
    "plt.plot(Depth,AccuracyTraining)\n",
    "plt.plot(Depth,AccuracyValidation)\n",
    "\n",
    "\n",
    "tree2 = model2\n",
    "\n",
    "#Confusion Matrix\n",
    "y_pred = model2.predict(X_validation)\n",
    "accuracy = accuracy_score(Y_validation, y_pred)\n",
    "AccuracyValidation.append(accuracy)\n",
    "print(accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "# your code here\n",
    "Y_validation\n",
    "print('\\nFor the validation set:')\n",
    "print('Accuracy: \\t{}'.format(accuracy_score(Y_validation, model2.predict(X_validation))))\n",
    "print('Precision: \\t{}'.format(precision_score(Y_validation, model2.predict(X_validation))))\n",
    "print('Recall: \\t{}'.format(recall_score(Y_validation, model2.predict(X_validation))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree as tree\n",
    "import graphviz\n",
    "dot_data = tree.export_graphviz(model2, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "\n",
    "predictors = X_train.columns\n",
    "export_graphviz (model2, out_file='tree.dot', \n",
    "                feature_names = predictors,\n",
    "                class_names = ('Negative', 'Positive'),\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n",
    "\n",
    "# Convert to png using system command (requires Graphviz)\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "Image(filename = 'tree.png')\n",
    "\n",
    "y_pred = model2.predict(X_validation)\n",
    "accuracy = accuracy_score(Y_validation, y_pred)\n",
    "AccuracyValidation.append(accuracy)\n",
    "print(accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(gamma='auto' ,C = 0.8)\n",
    "clf.fit(X_train, Y_train)\n",
    "y_pred = clf.predict(X_validation)\n",
    "accuracy = accuracy_score(Y_validation, y_pred)\n",
    "print(accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0.6455382436260623\n",
    "0.6329674220963173\n",
    "0.6296033994334278\n",
    "0.6247344192634561\n",
    "\n",
    "0.6412889518413598\n",
    "0.6412004249291785\n",
    "0.6400495750708215\n",
    "0.6395184135977338\n",
    "\n",
    "0.6412889518413598\n",
    "0.6427939093484419\n",
    "0.6448300283286119\n",
    "0.6455382436260623\n",
    "\n",
    "Worked well with a c value of about .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Doing it with the test data </h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.drop(columns=[\"name\",\"usd_pledged_real\",\"launched\",\"backers\",\"deadline\"],inplace=True)\n",
    "\n",
    "test = test[test.category != 'Fashion']\n",
    "\n",
    "X_test = test.drop(columns = \"StateBin\")\n",
    "Y_test = test[\"StateBin\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "modelList = [clf,tree2,Forrest]\n",
    "\n",
    "for model in modelList:\n",
    "    print(model)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(Y_test, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Bartys todo list:\n",
    "    Confusing matrix for everything\n",
    "    Final result for everything\n",
    "    Backers and not backers\n",
    "    What catagorys produce the best values\n",
    "    Accuracy values for everything with validation and test data\n",
    "    Random forrest with test data\n",
    "    model hypothsis\n",
    "    Accuracy if you dont have any analysis.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
