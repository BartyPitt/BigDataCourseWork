{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:Red;\"> Importing the libaries </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:Green;\"> Sanitisation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45585 entries, 0 to 45584\n",
      "Data columns (total 9 columns):\n",
      "name                45585 non-null object\n",
      "category            45585 non-null object\n",
      "deadline            45585 non-null object\n",
      "launched            45585 non-null object\n",
      "backers             45585 non-null int64\n",
      "country             45585 non-null object\n",
      "usd_pledged_real    45585 non-null float64\n",
      "usd_goal_real       45585 non-null float64\n",
      "StateBin            45585 non-null int64\n",
      "dtypes: float64(2), int64(2), object(5)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "FullSetClean = pd.read_csv(\"data_edited3.csv\")\n",
    "\n",
    "#####\n",
    "\"\"\"\n",
    "Put Further Sanitation Code here.\n",
    "OR any sanitation code\n",
    "\"\"\"\n",
    "FullSetClean = FullSetClean.drop(columns = \"currency\")\n",
    "\n",
    "stateBin = []\n",
    "for row in FullSetClean[\"state\"]:\n",
    "    if row == \"successful\":\n",
    "        stateBin.append(1)\n",
    "    else:\n",
    "        stateBin.append(0)\n",
    "\n",
    "FullSetClean[\"StateBin\"] = stateBin\n",
    "FullSetClean = FullSetClean.drop(columns = \"state\")\n",
    "FullSetClean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:Fuchsia;\"> The functions related to the words </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:Teal;\"> Letter Related Functions </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0.25\n",
      "0.6666666666666666\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def StartingChar(string : str): # discontinued as does not output a numerical number that could be used later.\n",
    "    return string[0]\n",
    "\n",
    "def Length(string : str):\n",
    "    return len(string)\n",
    "\n",
    "def NumberOfWords(string :  str):\n",
    "    output = 0\n",
    "    string = string.split()\n",
    "    for word in string:\n",
    "         if any(c.isalpha() for c in word): ##sees if there is a letter in the collection of chars\n",
    "                output += 1\n",
    "    return output\n",
    "\n",
    "def Capitilisation(string : str):\n",
    "    plus = 0\n",
    "    minus = 0\n",
    "    for char in string:\n",
    "        if char.islower():\n",
    "            plus +=1\n",
    "        elif char.isupper():\n",
    "            minus += 1\n",
    "    return (plus/(plus + minus))\n",
    "\n",
    "def Punctuation(string : str):\n",
    "    plus = 0\n",
    "    minus = 0\n",
    "    for char in string:\n",
    "        if char in \". , / ? ; : ‘ () !”\":\n",
    "            plus +=1\n",
    "        else:\n",
    "            minus += 1\n",
    "    return (plus/(plus + minus))\n",
    "\n",
    "def nonPunctuation(string:str):\n",
    "    plus = 0\n",
    "    minus = 0\n",
    "    for char in string:\n",
    "        if char in \"\\|£$%^&*-_+={}[]@~#<>¬\":\n",
    "            plus +=1\n",
    "        else:\n",
    "            minus += 1\n",
    "    return (plus/(plus + minus))\n",
    "\n",
    "def Vowels(string : str):\n",
    "    plus = 0\n",
    "    minus = 0\n",
    "    string = string.lower()\n",
    "    for char in string:\n",
    "        if char in \"aeiou\":\n",
    "            plus += 1\n",
    "        elif char.isalpha():\n",
    "            minus += 1\n",
    "    return (plus/(plus + minus))\n",
    "\n",
    "def Plositives(string : str):\n",
    "    plus = 0\n",
    "    minus = 0\n",
    "    string = string.lower()\n",
    "    for char in string:\n",
    "        if char in \"ptkbdg\":\n",
    "            plus += 1\n",
    "        elif char.isalpha():\n",
    "            minus += 1\n",
    "    return (plus/(plus + minus))\n",
    "\n",
    "def frictives(string : str):\n",
    "    plus = 0\n",
    "    minus = 0\n",
    "    string  = string.lower()\n",
    "    for char in string:\n",
    "        if char in \"fsvz\":\n",
    "            plus += 1\n",
    "        elif char.isalpha():\n",
    "            minus += 1\n",
    "    return (plus/(plus + minus))\n",
    "\n",
    "def alliteration(string : str):\n",
    "    output = 0\n",
    "    string = string.lower()\n",
    "    string = string.split()\n",
    "    previousLetter = \"\"\n",
    "    output = 0\n",
    "    for word in string:\n",
    "        if word[0] == previousLetter:\n",
    "                output += 1\n",
    "        else:\n",
    "            previousLetter = word[0]\n",
    "        \n",
    "    return output\n",
    "\n",
    "functionList = [\n",
    "                [Length , \"Word Length\"],\n",
    "                [NumberOfWords , \"Number Of Words\"],\n",
    "                [Capitilisation , \"Capitilisation\"],\n",
    "                [Punctuation , \"Punctuation\"],\n",
    "                [nonPunctuation , \"nonPunctuation\"],\n",
    "                [Vowels , \"Vowels\"],\n",
    "                [Plositives, \"Plositives\"],\n",
    "                [frictives,\"frictives\"],\n",
    "                [alliteration,\"alliteration\"]\n",
    "               ]\n",
    "    \n",
    "print(NumberOfWords(\"Test Test test a!\")) \n",
    "print (frictives(\"test test test\"))\n",
    "print (Capitilisation(\"TEst Test Test\"))\n",
    "print (alliteration(\"Fest Test test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At some point this will be changed to a genralised function.\n",
    "#base plan:\n",
    "#all the analytics function will take a string return a number (float or int 64 )\n",
    "\n",
    "#there will be an itterator high order function that takes the analytic function. \n",
    "#and the applies it to all of the titles and returns a list\n",
    "\n",
    "\n",
    "###The itterator high order\n",
    "\n",
    "def itterator(function,ColName,dataSet):\n",
    "    output = list()\n",
    "    for n , string in enumerate(dataSet[\"name\"]):\n",
    "        output.append(function(string))\n",
    "    dataSet[ColName] = output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function Length at 0x0000022DBF8DA400>\n",
      "<function NumberOfWords at 0x0000022DC20190D0>\n",
      "<function Capitilisation at 0x0000022DC2019048>\n",
      "<function Punctuation at 0x0000022DC0773158>\n",
      "<function nonPunctuation at 0x0000022DC1F4A2F0>\n",
      "<function Vowels at 0x0000022DC1FC0E18>\n",
      "<function Plositives at 0x0000022DC1FC0EA0>\n",
      "<function frictives at 0x0000022DC1FC0B70>\n",
      "<function alliteration at 0x0000022DC1FC0C80>\n",
      "done functions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for point in functionList:\n",
    "    itterator(point[0],point[1],FullSetClean)\n",
    "    print(point[0])\n",
    "\n",
    "print (\"done functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:Orange;\"> Time Related Functions </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def dateTimeitterator(dataSet):\n",
    "    Launchweekdays = []\n",
    "    Launchhours = []\n",
    "    elapsedDays= []\n",
    "    deadlineweekdays = []\n",
    "    draw = []\n",
    "    sraw = []\n",
    "    \n",
    "    for n , string in enumerate(dataSet[\"launched\"]):\n",
    "        date , time = string.split(\" \")\n",
    "        day , month ,year = date.split(\"/\")\n",
    "        hour, mineut = time.split(\":\")\n",
    "        raw = datetime.date(int(year), int(month), int(day))\n",
    "\n",
    "        \n",
    "        Launchweekdays.append(int(raw.weekday()))\n",
    "        Launchhours.append(int(hour))\n",
    "        sraw.append(raw)\n",
    "        \n",
    "    for n , string in enumerate(dataSet[\"deadline\"]):\n",
    "        \n",
    "        day , month ,year = string.split(\"/\")\n",
    "        hour, mineut = time.split(\":\")\n",
    "        raw = datetime.date(int(year), int(month), int(day))\n",
    "        deadlineweekdays.append(raw.weekday())\n",
    "        draw.append(raw)\n",
    "        \n",
    "\n",
    "    elapsedDays = [(a - b).days for a, b in zip(sraw, draw)]\n",
    "    dataSet[\"LaunchWeekday\"] = Launchweekdays\n",
    "    dataSet[\"LaunchHour\"] =  Launchhours\n",
    "    dataSet[\"elapsedDay\"] = elapsedDays\n",
    "    dataSet[\"deadlineWeekday\"] = deadlineweekdays\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done Timeritterato\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dateTimeitterator(FullSetClean)\n",
    "print (\"done Timeritterato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:salmon;\"> Data Base Splitting (to be added data base correction) + Dataset Balancing </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data base splitting done bellow \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train,other = train_test_split(FullSetClean, test_size=0.6,random_state=0);\n",
    "\n",
    "validation , test = train_test_split(other, test_size=0.5,random_state=0);\n",
    "\n",
    "train.head()\n",
    "\n",
    "# FullSetClean.head()\n",
    "# FullSetClean.info(verbose = True)\n",
    "\n",
    "# BALANCES THE TRAINING DATASET:\n",
    "\n",
    "total = len(train)\n",
    "nb_pos = train['StateBin'].sum()\n",
    "nb_neg = total - nb_pos\n",
    "\n",
    "success_pos = train.loc[train['StateBin'] == 1]\n",
    "success_neg = train.loc[train['StateBin'] == 0].sample(nb_pos)\n",
    "\n",
    "resampled_train = pd.concat((success_pos, success_neg))\n",
    "\n",
    "# CHECKS THAT RESAMPLING HAS BEEN SUCCESSFUL:\n",
    "\n",
    "# total = len(resampled_train)\n",
    "# nb_pos = resampled_train['StateBin'].sum()\n",
    "# nb_neg = total - nb_pos\n",
    "\n",
    "# print('Successful: {}' .format(nb_pos))\n",
    "# print('Failed: {}' .format(nb_neg))\n",
    "\n",
    "# BALANCES THE VALIDATION DATASET:\n",
    "\n",
    "total = len(validation)\n",
    "nb_pos = validation['StateBin'].sum()\n",
    "nb_neg = total - nb_pos\n",
    "\n",
    "success_pos = validation.loc[validation['StateBin'] == 1]\n",
    "success_neg = validation.loc[validation['StateBin'] == 0].sample(nb_pos)\n",
    "\n",
    "resampled_validation = pd.concat((success_pos, success_neg))\n",
    "\n",
    "# CHECKS THAT RESAMPLING HAS BEEN SUCCESSFUL:\n",
    "\n",
    "# total = len(resampled_validation)\n",
    "# nb_pos = resampled_validation['StateBin'].sum()\n",
    "# nb_neg = total - nb_pos\n",
    "\n",
    "# print('Successful: {}' .format(nb_pos))\n",
    "# print('Failed: {}' .format(nb_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:Purple;\"><i> Now for the word proccessing </i></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ghgh', 'ghgh', 'fdkfkf', 'g']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def sanitiser(title : str) -> list: #takes a string splits into words , makes lower case and removes punctuation\n",
    "    words = title.split()\n",
    "    output = []\n",
    "    for word in words:\n",
    "       output.append(re.sub(r'\\W+', '', word).lower())\n",
    "    return output\n",
    "\n",
    "def WordFinder(dataSet,cutoff : int) -> dict and list: #so takes in the data set , a cut off an renturns a list of all words above the cutoff and their percentage chance of succsess\n",
    "    LargeWordDictionary = {} #The dict for all of the words \n",
    "    \n",
    "    for n , title in enumerate(dataSet[\"name\"]): #goes through the big old list\n",
    "        temp = sanitiser(title) #sanitises the function\n",
    "        for word in temp: #basically sees if the word is already in the large list of words if it is then it adds its location in the data base to the end of the dict entry\n",
    "            try:\n",
    "                LargeWordDictionary[word].append(n)\n",
    "            except KeyError:\n",
    "                LargeWordDictionary[word] = [n]\n",
    "                \n",
    "    SmallWordDictionary = {} #small output dictionary\n",
    "    StateBin = dataSet[\"StateBin\"].tolist() #transfers database to list due to pandas related issues\n",
    "    for word in LargeWordDictionary: # goes through large dictionary , counts number of instances of each word appearing , and then uses the pointers to find if they were success \n",
    "        Suc = 0\n",
    "        for pointer in LargeWordDictionary[word]:\n",
    "            Suc += StateBin[int(pointer)]\n",
    "        length = len(LargeWordDictionary[word])\n",
    "        if length >= cutoff:\n",
    "            SmallWordDictionary[word] = [length , Suc/length]\n",
    "    print(len(SmallWordDictionary))\n",
    "    \n",
    "    WordLevels = [[a,[]] for a in range(10)]\n",
    "    for word in SmallWordDictionary:\n",
    "        WordLevels[int(SmallWordDictionary[word][1]*10)][1].append(word)\n",
    "    \n",
    "    return SmallWordDictionary , WordLevels\n",
    "        \n",
    "    #return LargeWordDictionary\n",
    "\n",
    "penis = 80083\n",
    "\n",
    "def WordScore(title : str,AssementDictionary : dict ,split : int) ->list:\n",
    "    title = sanitiser(title)\n",
    "    output = [0 for i in range(split)]\n",
    "    unique = 0\n",
    "    for word in title:\n",
    "        try:\n",
    "            temp = AssementDictionary[word][1]\n",
    "            for i in range(split):\n",
    "                if temp > 1 *((i+1)/split):\n",
    "                    pass\n",
    "                else:\n",
    "                    output[i] += 1\n",
    "                    break\n",
    "        except KeyError:\n",
    "            unique += 1\n",
    "    return output , unique\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "sanitiser(\"ghgh ghgh!!!!  FDKFKF  g\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193\n",
      "[[0, ['app', 'truck']], [1, ['dream', 'fashion', 'mobile', 'clothing', 'online']], [2, ['my', 'de', 'through', 'natural', 'organic', 'unique', 'social', 'community', 'shop', 'food', 'me', 'american', 'co', 'free', 'family', 'apparel', 'real', 'fun']], [3, ['go', 'documentary', 'series', 'big', 'is', 'music', 'system', 'for', 'to', 'handmade', 'help', 'el', 'world', 'design', 'life', 'project', 'tour', 'up', 'experience', 'coffee', 'live', 'kids', 'way', 'original', 'case', 'story', 'en', 'launch', 'people', 'studio', 'box', 'all', 'it', 'inspired', 'video', 'i', 'movie', 'us', 'school', 'time', 'travel', 'get', 'company', 'phone', 'modern', 'water', 'dog']], [4, ['of', 'a', 'christmas', 'the', '', 'one', 'and', 'art', '3d', 'your', 'with', 'that', 'great', 'be', 'new', 'make', 'on', 'custom', 'in', 'magazine', 'good', 'back', '100', 'more', 'day', 'fund', 'no', 'game', 'la', 'little', 'home', 'we', 'play', 'board', 'card', 'you', 'feature', 'our', 'support', 'love', 'bag', 'first', 'games', 'best', 'making', 'gaming']], [5, ['black', 'record', 'next', 'film', 'book', 'worlds', 'most', 'at', 'an', 'leather', 'are', 'childrens', 'song', 'vinyl', 'light', 'from', 'show', 'horror', 'about', 'future', 'ultimate', 'hard', 'collection', 'ever', 'smart', 'dance', 'fantasy', 'women', 'space', 'bike', 'ep', 'novel', 'war', 'watch', 'cd', 'man', 'made', 'girl', 'print', '4', 'high', 'watches', 'red']], [6, ['festival', '2018', 'by', 'album', 'dark', 'cards', '2', '1', '3', 'enamel', 'wallet', 'pins', 'short', '2017', 'comic', 'set', 'stories', 'rpg', 'calendar', 'adventure', 'cat']], [7, ['deck', 'graphic', 'playing', 'pin', 'debut', 'edition', 'volume']], [8, ['issue', 'presents', 'dice']], [9, ['miniatures']]]\n",
      "([0, 0, 1, 0, 0, 0, 0, 0, 0, 1], 1)\n"
     ]
    }
   ],
   "source": [
    "testAll,TestClasses = WordFinder(test,40)\n",
    "\n",
    "print(TestClasses)\n",
    "\n",
    "print (WordScore(\"miniatures blank food\",testAll,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:Navy;\"> Machine Learning Code Taken From Tutorial </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def select_column_to_add(X_train, y_train, X_val, y_val, columns_in_model, columns_to_test):\n",
    "    \n",
    "    column_best = None\n",
    "    columns_in_model = list(columns_in_model)\n",
    "    \n",
    "    if len(columns_in_model) == 0:\n",
    "        acc_best = 0\n",
    "    elif len(columns_in_model) == 1:\n",
    "        mod = LogisticRegression(C=1e9).fit(X_train[columns_in_model].values.reshape(-1, 1), y_train)\n",
    "        acc_best = accuracy_score(y_val, mod.predict(X_val[columns_in_model].values.reshape(-1, 1)))\n",
    "    else:\n",
    "        mod = LogisticRegression(C=1e9).fit(X_train[columns_in_model], y_train)\n",
    "        acc_best = accuracy_score(y_val, mod.predict(X_val[columns_in_model]))\n",
    "\n",
    "    \n",
    "    for column in columns_to_test:\n",
    "        mod = LogisticRegression(C=1e9).fit(X_train[columns_in_model+[column]], y_train)\n",
    "        y_pred = mod.predict(X_val[columns_in_model+[column]])\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        \n",
    "        if acc - acc_best >= 0.005:  # one of our stopping criteria\n",
    "            acc_best = acc\n",
    "            column_best = column\n",
    "        \n",
    "    if column_best is not None:  # the other stopping criteria\n",
    "        print('Adding {} to the model'.format(column_best))\n",
    "        print('The new best validation accuracy is {}'.format(acc_best))\n",
    "        columns_in_model_updated = columns_in_model + [column_best]\n",
    "    else:\n",
    "        print('Did not add anything to the model')\n",
    "        columns_in_model_updated = columns_in_model\n",
    "    \n",
    "    return columns_in_model_updated, acc_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18234 entries, 27204 to 2732\n",
      "Data columns (total 13 columns):\n",
      "Word Length        18234 non-null int64\n",
      "Number Of Words    18234 non-null int64\n",
      "Capitilisation     18234 non-null float64\n",
      "Punctuation        18234 non-null float64\n",
      "nonPunctuation     18234 non-null float64\n",
      "Vowels             18234 non-null float64\n",
      "Plositives         18234 non-null float64\n",
      "frictives          18234 non-null float64\n",
      "alliteration       18234 non-null int64\n",
      "LaunchWeekday      18234 non-null int64\n",
      "LaunchHour         18234 non-null int64\n",
      "elapsedDay         18234 non-null int64\n",
      "deadlineWeekday    18234 non-null int64\n",
      "dtypes: float64(6), int64(7)\n",
      "memory usage: 1.9 MB\n"
     ]
    }
   ],
   "source": [
    "dropstuff = [\"StateBin\",\"name\",\"category\",\"deadline\",\"launched\",\"country\",\"usd_goal_real\",\"usd_pledged_real\",\"backers\"]\n",
    "\n",
    "Ytrain = train[[\"StateBin\"]]  #what you want to predict\n",
    "Xtrain = train.drop(columns = dropstuff) #all the other data\n",
    "Yval = validation[[\"StateBin\"]]  #what you want to predict\n",
    "Xval = validation.drop(columns = dropstuff) #all the other data\n",
    "\n",
    "Xtrain.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding frictives to the model\n",
      "The new best validation accuracy is 0.5869835466179159\n",
      "Did not add anything to the model\n",
      "Did not add anything to the model\n",
      "Did not add anything to the model\n"
     ]
    }
   ],
   "source": [
    "columns_to_test = set(Xtrain)\n",
    "full_list = set(Xtrain)\n",
    "\n",
    "columns_in_model = list();\n",
    "\n",
    "for i in range(4):\n",
    "    columns_in_model , acc = select_column_to_add(Xtrain,Ytrain,Xval,Yval,columns_in_model,columns_to_test)    \n",
    "    columns_to_test = full_list.difference(columns_in_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:DodgerBlue;\"> Now For the Graphs </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentageAtIntervalFloat(dataSet ,varible : str,splits : int) -> list :\n",
    "    #create the search thing\n",
    "    minimum = dataSet[varible].min()\n",
    "    maximum = dataSet[varible].max()\n",
    "    print(minimum , maximum)\n",
    "    difference = maximum - minimum\n",
    "    space = difference / splits\n",
    "    output = [[a,[0,0]] for a in np.arange(minimum,maximum,space)]\n",
    "    testLevels = [a for a in np.arange(minimum,maximum,space)]\n",
    "    Values = dataSet[varible].tolist()\n",
    "    successes = dataSet[\"StateBin\"].tolist()\n",
    "    \n",
    "    for Value , success in zip(Values,successes):\n",
    "        for n , level in enumerate(testLevels):\n",
    "            if Value <= level:\n",
    "                if  success == 1:\n",
    "                    output[n][1][1] += 1\n",
    "                else:\n",
    "                    output[n][1][0] += 1\n",
    "                break\n",
    "    \n",
    "    percentOutput = []\n",
    "    for level in output:\n",
    "        if level[1][1] == 0:\n",
    "            percentOutput.append(0)\n",
    "        else:\n",
    "            percentOutput.append(level[1][1]/(level[1][1]+level[1][0]))\n",
    "    return percentOutput\n",
    "\n",
    "def percentageAtIntervalInt(dataSet ,varible : str) -> list : ###Recode so that it works with intergers\n",
    "    #create the search thing\n",
    "    minimum = dataSet[varible].min()\n",
    "    maximum = dataSet[varible].max()\n",
    "    difference = maximum - minimum\n",
    "    output = [[a,[0,0]] for a in np.arange(minimum,maximum)]\n",
    "    testLevels = [a for a in np.arange(minimum,maximum)]\n",
    "    Values = dataSet[varible].tolist()\n",
    "    successes = dataSet[\"StateBin\"].tolist()\n",
    "    \n",
    "    for Value , success in zip(Values,successes):\n",
    "        for n , level in enumerate(testLevels):\n",
    "            if Value <= level:\n",
    "                if  success == 1:\n",
    "                    output[n][1][1] += 1\n",
    "                else:\n",
    "                    output[n][1][0] += 1\n",
    "                break\n",
    "    \n",
    "    percentOutput = []\n",
    "    for level in output:\n",
    "        if level[1][1] == 0:\n",
    "            percentOutput.append(0)\n",
    "        else:\n",
    "            percentOutput.append(level[1][1]/(level[1][1]+level[1][0]))\n",
    "    return percentOutput\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Length\n",
      "1 60\n",
      "Number Of Words\n",
      "1 16\n",
      "Capitilisation\n",
      "0.0 1.0\n",
      "Punctuation\n",
      "0.0 0.5666666666666667\n",
      "nonPunctuation\n",
      "0.0 0.46153846153846156\n",
      "Vowels\n",
      "0.0 1.0\n",
      "Plositives\n",
      "0.0 1.0\n",
      "frictives\n",
      "0.0 0.6666666666666666\n",
      "alliteration\n",
      "0 6\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "test[\"Vowels\"].max()\n",
    "for n , Fun in enumerate(functionList):\n",
    "\n",
    "    print(Fun[1])\n",
    "    LookList = percentageAtIntervalFloat(FullSetClean , Fun[1],20)\n",
    "    plt.subplot(3,3,n + 1)\n",
    "    plt.plot(LookList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
